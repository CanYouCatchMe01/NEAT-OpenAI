{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCar\n",
    "\n",
    "\n",
    "### Observation Space\n",
    "The observation is a `ndarray` with shape `(2,)` where the elements correspond to the following:\n",
    "| Num | Observation                          | Min  | Max | Unit         |\n",
    "|-----|--------------------------------------|------|-----|--------------|\n",
    "| 0   | position of the car along the x-axis | -Inf | Inf | position (m) |\n",
    "| 1   | velocity of the car                  | -Inf | Inf | position (m) |\n",
    "### Action Space\n",
    "There are 3 discrete deterministic actions:\n",
    "| Num | Observation             | Value | Unit         |\n",
    "|-----|-------------------------|-------|--------------|\n",
    "| 0   | Accelerate to the left  | Inf   | position (m) |\n",
    "| 1   | Don't accelerate        | Inf   | position (m) |\n",
    "| 2   | Accelerate to the right | Inf   | position (m) |\n",
    "### Transition Dynamics:\n",
    "Given an action, the mountain car follows the following transition dynamics:\n",
    "*velocity<sub>t+1</sub> = velocity<sub>t</sub> + (action - 1) * force - cos(3 * position<sub>t</sub>) * gravity*\n",
    "*position<sub>t+1</sub> = position<sub>t</sub> + velocity<sub>t+1</sub>*\n",
    "where force = 0.001 and gravity = 0.0025. The collisions at either end are inelastic with the velocity set to 0\n",
    "upon collision with the wall. The position is clipped to the range `[-1.2, 0.6]` and\n",
    "velocity is clipped to the range `[-0.07, 0.07]`.\n",
    "### Reward:\n",
    "The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is\n",
    "penalised with a reward of -1 for each timestep.\n",
    "### Starting State\n",
    "The position of the car is assigned a uniform random value in *[-0.6 , -0.4]*.\n",
    "The starting velocity of the car is always assigned to 0.\n",
    "### Episode End\n",
    "The episode ends if either of the following happens:\n",
    "1. Termination: The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "2. Truncation: The length of the episode is 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import keyboard\n",
    "env = gym.make(\"MountainCar-v0\", new_step_api=True, render_mode='human')\n",
    "env._max_episode_steps = 1000\n",
    "\n",
    "# Play\n",
    "state = env.reset()  # [position, velocity]\n",
    "terminated = False # True if the player dies or wins\n",
    "truncated = False # True if the time runs out\n",
    "score = 0\n",
    "\n",
    "while not terminated and not truncated:\n",
    "    #Close window with ESC key\n",
    "    if keyboard.is_pressed('esc'):\n",
    "        env.close()\n",
    "        assert False\n",
    "\n",
    "    # Read keyboard input\n",
    "    int_action = 1  # No push\n",
    "    if keyboard.is_pressed('right'):\n",
    "        int_action = 2  # right push\n",
    "    elif keyboard.is_pressed('left'):\n",
    "        int_action = 0  # left push\n",
    "\n",
    "    # Send input to game\n",
    "    state, reward, terminated, truncated, info = env.step(int_action)\n",
    "\n",
    "    score += reward\n",
    "print(score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI enviroment setup\n",
    "run once before running the \"AI test\" and \"AI train block\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import neat\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "def convert_to_int_action(action):\n",
    "    int_action = 1  # No push\n",
    "\n",
    "    if action[0] > 0.5:\n",
    "        int_action = 2  # Right push\n",
    "    elif action[1] > 0.5:\n",
    "        int_action = 0  # Left push\n",
    "    return int_action\n",
    "\n",
    "# Getting file with values\n",
    "config_path = os.path.abspath(\"config-feedforward.txt\")\n",
    "config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction, neat.DefaultSpeciesSet, neat.DefaultStagnation, config_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train AI\n",
    "Remember to run \"AI enviroment setup\" before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_train(genomes, config):\n",
    "    nets = []  # Neural networks/brain\n",
    "    ge = []  # Genes\n",
    "\n",
    "    # Setting up AI\n",
    "    for _, g in genomes:\n",
    "        net = neat.nn.FeedForwardNetwork.create(g, config)  # Getting values from txt\n",
    "        nets.append(net)  # Appending every Neural Network to array\n",
    "        g.fitness = 0\n",
    "        ge.append(g)  # Appending every gene to array.\n",
    "\n",
    "    # training every agent\n",
    "    for g, net in zip(ge, nets):\n",
    "        # Start game\n",
    "        state = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Variables for calculating fitness\n",
    "        high_vel = -math.inf\n",
    "        low_vel = math.inf\n",
    "        high_pos = -math.inf\n",
    "        low_pos = math.inf\n",
    "        elapsed_steps = 0\n",
    "        closest_distance = math.inf\n",
    "\n",
    "        time_reward = 0\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            action = net.activate((state))  # send state to AI\n",
    "\n",
    "            int_action = convert_to_int_action(action)\n",
    "            state, reward, terminated, truncated, info = env.step(int_action)\n",
    "            \n",
    "            pos, vel = state\n",
    "            elapsed_steps += 1\n",
    "\n",
    "            #if pos > high_pos:\n",
    "              #  g.fitness += 1\n",
    "             #   high_pos = pos\n",
    "            #if pos < low_pos:\n",
    "            #    g.fitness += 1\n",
    "           #     low_pos = pos\n",
    "\n",
    "            #high_vel = max(high_vel, vel)\n",
    "            #low_vel = min(low_vel, vel)\n",
    "            #high_pos = max(high_pos, pos)\n",
    "            #low_pos = min(low_pos, pos)\n",
    "            #closest_distance = min(closest_distance, abs(pos - 0.6))\n",
    "\n",
    "            #big_vel_swing = (high_vel - low_vel)\n",
    "            #big_pos_swing = (high_pos - low_pos)\n",
    "            #g.fitness = big_pos_swing / elapsed_steps\n",
    "            #g.fitness = (big_vel_swing - 1/closest_distance) / elapsed_steps\n",
    "            #g.fitness += vel\n",
    "            #g.fitness += big_pos_swing / elapsed_steps\n",
    "        #g.fitness /= elapsed_steps\n",
    "\n",
    "            #inverted_distance = 1 / distance\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make(\"MountainCar-v0\", new_step_api=True)\n",
    "env._max_episode_steps = 1000\n",
    "\n",
    "p = neat.Population(config)\n",
    "p.add_reporter(neat.StdOutReporter(True))  # printing stats\n",
    "\n",
    "num_training_generations = 10\n",
    "winner = p.run(ai_train, num_training_generations)\n",
    "print(winner)\n",
    "\n",
    "# Save best gene in Pickle file\n",
    "file = open('winner_neat.p', 'wb')\n",
    "pickle.dump(winner, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test AI\n",
    "Remember to run \"AI enviroment setup\" before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyboard\n",
    "\n",
    "def ai_test():\n",
    "    # open pickle file\n",
    "    file = open('winner_neat.p', 'rb')\n",
    "    winner = pickle.load(file)\n",
    "    file.close()\n",
    "    print(winner)\n",
    "\n",
    "    # Create Neural Network\n",
    "    winner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n",
    "\n",
    "    state = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        #Close window with ESC key\n",
    "        if keyboard.is_pressed('esc'):\n",
    "            env.close()\n",
    "            assert False\n",
    "\n",
    "        action = winner_net.activate((state))\n",
    "\n",
    "        int_action = convert_to_int_action(action)\n",
    "        state, reward, terminated, truncated, info = env.step(int_action)\n",
    "        score += reward\n",
    "    print(score)\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make(\"MountainCar-v0\", new_step_api=True, render_mode='human')\n",
    "env._max_episode_steps = 1000\n",
    "\n",
    "p = neat.Population(config)\n",
    "p.add_reporter(neat.StdOutReporter(True))  # printing stats\n",
    "\n",
    "ai_test()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('ai1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "22adca1485c066d5be5e298dc527b36b8b91d253fc851e84e84a6bf1cc1998cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
